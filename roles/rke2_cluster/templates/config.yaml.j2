{% if rke2_config.write_kubeconfig_mode is defined %}
write-kubeconfig-mode: "{{ rke2_config.write_kubeconfig_mode }}"
{% endif %}
debug : true
cni: cilium
disable-network-policy: true
# Cluster member identification
node-name: {{ inventory_hostname }}

# All cluster members must be listed in tls-san
tls-san:
  # Standard entries
  - "127.0.0.1"
  - "kubernetes"
  - "kubernetes.default"
  # Hostnames for all nodes
{% for host in groups['control_plane_nodes'] %}
  - "{{ host }}"  # Hostname
  - "{{ hostvars[host]['ansible_host'] }}"  # IP address
{% endfor %}
{% if groups['worker_nodes'] is defined %}
{% for host in groups['worker_nodes'] %}
  - "{{ host }}"  # Hostname
  - "{{ hostvars[host]['ansible_host'] }}"  # IP address
{% endfor %}
{% endif %}

# Cluster formation configuration
{% if inventory_hostname == groups['control_plane_nodes'][0] %}
# First control plane node initializes the cluster
cluster-init: true
{% else %}
# Other nodes join the first control plane
server: https://{{ hostvars[groups['control_plane_nodes'][0]]['ansible_host'] }}:9345
{% endif %}

# Cluster join token
token: {{ rke2_token }}

# Node role labels and taints
node-label:
{% if inventory_hostname in groups['control_plane_nodes'] %}
  - "node.kubernetes.io/instance-type=control-plane"
  - "kubernetes.io/hostname={{ inventory_hostname }}"
  - "workload.type=control-plane"
#removed this because it was causing the nodes to not start
#node-taint:
#  - "CriticalAddonsOnly=true:NoSchedule"
{% else %}
  - "node.kubernetes.io/instance-type=worker"
  - "kubernetes.io/hostname={{ inventory_hostname }}"
  - "workload.type=mixed"
{% endif %}

# Additional cluster members (for service discovery)
cluster-domain: cluster.local
additional-sans:
{% for host in groups['control_plane_nodes'] %}
  - "{{ host }}"
  - "{{ hostvars[host]['ansible_host'] }}"
{% endfor %}
{% if groups['worker_nodes'] is defined %}
{% for host in groups['worker_nodes'] %}
  - "{{ host }}"
  - "{{ hostvars[host]['ansible_host'] }}"
{% endfor %}
{% endif %}
